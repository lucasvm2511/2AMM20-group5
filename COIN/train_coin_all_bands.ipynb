{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-25T11:51:44.555138Z",
     "start_time": "2024-10-25T11:51:25.063975Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from src.data import NumpyDatasetAllBands\n",
    "from src.model import Siren\n",
    "from src.utils import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:53:12.534684Z",
     "start_time": "2024-10-25T11:53:12.530146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = NumpyDatasetAllBands(\"data/np_downscaled_and_cropped\", transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, pin_memory=True, num_workers=0, shuffle=True)"
   ],
   "id": "42de2478ee1dc26d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:53:13.567147Z",
     "start_time": "2024-10-25T11:53:13.561558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_siren(model, model_input, ground_truth):\n",
    "    # Model Parameters\n",
    "    total_steps = 10_000\n",
    "    steps_til_summary = 100\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 50\n",
    "    min_delta = 1e-4\n",
    "\n",
    "    optim = torch.optim.Adam(lr=1e-3, params=model.parameters())\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    steps_since_improvement = 0\n",
    "\n",
    "    for step in range(total_steps):\n",
    "        model_output, coords = model(model_input)    \n",
    "        loss = ((model_output - ground_truth) ** 2).mean()\n",
    "        \n",
    "        if loss + min_delta < best_loss:\n",
    "            best_loss = loss\n",
    "            steps_since_improvement = 0\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "    \n",
    "        if steps_since_improvement >= patience:\n",
    "            print(f\"Stopping early at step {step} due to no improvement.\")\n",
    "            break\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n"
   ],
   "id": "12ee5a951f77a381",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T11:54:09.674295Z",
     "start_time": "2024-10-25T11:53:14.120574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_qualities = []\n",
    "all_sizes = []\n",
    "\n",
    "image_counter = 1\n",
    "for x, y in dataloader:\n",
    "    qualities = []\n",
    "    sizes = []\n",
    "\n",
    "    counter = 1\n",
    "    for n_layers in [1, 2, 3]:\n",
    "        for n_hidden in range(5, 201, 5):\n",
    "            model = Siren(in_features=2,\n",
    "                          out_features=13,\n",
    "                          hidden_features=n_hidden, \n",
    "                          hidden_layers=n_layers,\n",
    "                          outermost_linear=True).to(device)\n",
    "\n",
    "\n",
    "            model_input, ground_truth = x.to(device), y.to(device)\n",
    "            print(f\"image {image_counter}; model {counter}/{3 * len(range(5, 201, 5))}\", end='\\t')\n",
    "            train_siren(model, model_input, ground_truth.to(device))\n",
    "\n",
    "            compressed = postprocess_model_output(model(model_input)[0])\n",
    "            real = postprocess_model_output(ground_truth)\n",
    "\n",
    "            quality = psnr(compressed, real).item()\n",
    "            bits_per_pixel = bpp(model) \n",
    "            # n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "            qualities.append(quality)\n",
    "            sizes.append(bits_per_pixel)\n",
    "            counter += 1\n",
    "\n",
    "    image_counter += 1\n",
    "    all_qualities.append(qualities)\n",
    "    all_sizes.append(sizes)"
   ],
   "id": "c839317d53201730",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sviatoslavgladkykh/miniconda/envs/myenv/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1; model 1/120\tStopping early at step 316 due to no improvement.\n",
      "image 1; model 2/120\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m model_input, ground_truth \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device), y\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage_counter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m; model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcounter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m3\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m201\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;241m5\u001B[39m))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m \u001B[43mtrain_siren\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mground_truth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m compressed \u001B[38;5;241m=\u001B[39m postprocess_model_output(model(model_input)[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     24\u001B[0m real \u001B[38;5;241m=\u001B[39m postprocess_model_output(ground_truth)\n",
      "Cell \u001B[0;32mIn[6], line 25\u001B[0m, in \u001B[0;36mtrain_siren\u001B[0;34m(model, model_input, ground_truth)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     23\u001B[0m     steps_since_improvement \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m steps_since_improvement \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m patience:\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStopping early at step \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m due to no improvement.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "np.save(\"artifacts/sizes_all_bands.npy\", all_qualities)\n",
    "np.save(\"artifacts/qualities_all_bands.npy\", all_qualities)"
   ],
   "id": "9f0d3b63fa0f1a2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "84d4807bb17abb33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bdf5fc902913498e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff024273e40d2794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eb09cd7649e28dec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9da7e0caa7114539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ce39502e8850895a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d097f0862b9962a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
