{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training a hypernetwork locally\n",
    "This notebook provides an example of how to create a hypernetwork and train it locally using the tools in this repo. We'll train the hypernetwork on MNIST, and we'll use the architecture created in hypernetwork_examples/hypernetwork_ae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-martinus-koop\u001b[0m (\u001b[33mnld\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2024-09-14 14:11:55.214586: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "import pprint\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.architecture = './model_components'\n",
    "# this time however, part of the network architecture lives somewhere else:\n",
    "config.model_type = ('./hypernetwork_example/hypernetwork_ae.py', 'Hypernetwork')  # we specify the module and the class within that module\n",
    "\n",
    "config.model_config = dict(\n",
    "    in_features = 1,\n",
    "    conv_features = 64,\n",
    "    shared_features = 2048,\n",
    "    mlp_hidden_features = 2048,\n",
    "    mlp_depth = 2,\n",
    "    inr_hidden_size = 64,\n",
    "    inr_depth = 3,\n",
    "    low_rank = 10,\n",
    "    kernel_size = 3,\n",
    "    groups = 1,\n",
    "    layer_type = \"inr_layers.SirenLayer\",\n",
    "    layer_kwargs = {'w0': 12},\n",
    ")\n",
    "\n",
    "# next, we set up  the training loop\n",
    "config.trainer_module = './hypernetwork_utils/'\n",
    "config.trainer_type = 'training.Trainer'\n",
    "config.train_loader = ('./hypernetwork_example/mnist.py', 'get_train_loader')\n",
    "config.validation_loader = ('./hypernetwork_example/mnist.py', 'get_validation_loader')\n",
    "config.batch_size = 16  # batch size for data loaders\n",
    "config.shuffle = True\n",
    "config.loss_function = 'inr_utils.losses.scaled_mse_loss'  # inr_utils is imported by hypernetwork_utils\n",
    "config.location_sampler = ('inr_utils.sampling.GridSubsetSampler',{  # NB when doing this (str, dict) thing,\n",
    "    # where the dict determines the config options for the thing str points to,\n",
    "    # default values of the thing str points to will take presedence over values \n",
    "    # specified in config (but not in dict)\n",
    "    'size': [28, 28],\n",
    "    'batch_size': 400,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "config.target = 'inr_utils.images.ArrayInterpolator'\n",
    "config.target_config = {\n",
    "    'interpolation_method': 'inr_utils.images.make_piece_wise_constant_interpolation',\n",
    "    'scale_to_01': False,  # this is already handled by the dataloader\n",
    "    'channels_first': True,  # because the dataloader puts channels first\n",
    "}\n",
    "\n",
    "config.optimizer = \"training.OptimizerFactory.single_optimizer\"\n",
    "config.optimizer_type = 'adamw'  # don't forget to add optax to additional default modules\n",
    "config.optimizer_config = {\n",
    "    #'learning_rate': 0.000015,  # is handled by the learning_rate_schedule\n",
    "    'weight_decay': 0.,\n",
    "}\n",
    "config.learning_rate_schedule = ('exponential_decay', { \n",
    "    'init_value': 0.000015,\n",
    "    'transition_steps': 12_000,\n",
    "    'decay_rate': .9,\n",
    "    'transition_begin': 12_000\n",
    "})\n",
    "\n",
    "config.sub_steps_per_datapoint = 2  # for every batch of images, take two gradient update steps with different coordinates\n",
    "config.epochs = 20\n",
    "\n",
    "config.metric_collector = 'metrics.MetricCollector'\n",
    "config.metric_collector_config = Config()\n",
    "config.metric_collector_config.batch_frequency = 600\n",
    "config.metric_collector_config.epoch_frequency = 2  # compute some metrics after every 2 epochs, e.g. because PlotOnGrid2D is slow (it probably isn't, but PlotOnGrid3D definitely is)\n",
    "config.metric_collector_config.metrics = [\n",
    "    'training.ValidationLoop',\n",
    "    ('metrics.PlotOnGrid2D', {'grid': 28, 'batch_size': 0, 'frequency': 'every_n_epochs', 'requires_scaling': True}),\n",
    "    ('metrics.MSEOnFixedGrid', {'grid': 28, 'num_dims': 2, 'batch_size': 0, 'frequency': 'every_n_batches'}),\n",
    "    ('metrics.LossStandardDeviation', {'window_size': 100, 'frequency': 'every_batch'})\n",
    "]\n",
    "\n",
    "config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "config.after_step_callback_config = {\n",
    "    'callbacks': [\n",
    "        ('callbacks.print_loss', {'after_every': 400}),\n",
    "        'callbacks.raise_error_on_nan'\n",
    "    ]\n",
    "}\n",
    "config.after_epoch_callback = None\n",
    "config.use_wandb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    model = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=True, # because this time much of the model is not in the module specified by 'architecture'\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPINR(\n",
       "  input_layer=SirenLayer(\n",
       "    weights=f32[64,2],\n",
       "    biases=f32[64],\n",
       "    activation_kwargs={'w0': 12}\n",
       "  ),\n",
       "  hidden_layers=[\n",
       "    SirenLayer(weights=f32[64,64], biases=f32[64], activation_kwargs={'w0': 12}),\n",
       "    SirenLayer(weights=f32[64,64], biases=f32[64], activation_kwargs={'w0': 12})\n",
       "  ],\n",
       "  output_layer=Linear(weights=f32[1,64], biases=f32[1], activation_kwargs={}),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(jnp.zeros((1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.10312119], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(jnp.zeros((1, 28, 28)))(jnp.zeros((2,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer = cju.run_utils.get_experiment_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_kwarg_in_trainer='hypernetwork',  # Trainer.__init__ takes a `hypernetwork` parameter to which the model that is to be trained should be passed\n",
    "        trainer_default_module_key='trainer_module',\n",
    "        additional_trainer_default_modules=[optax],\n",
    "        add_model_module_to_architecture_default_module=True,\n",
    "        model_sub_config_name_base='model',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's run it while logging to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/simon/Documents/inr_edu_24/wandb/run-20240914_141202-isds2pb9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nld/inr_edu_24/runs/isds2pb9' target=\"_blank\">fine-elevator-38</a></strong> to <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">https://wandb.ai/nld/inr_edu_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nld/inr_edu_24/runs/isds2pb9' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/isds2pb9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 20 epochs with 3750 batches per epoch and 2 gradient steps per batch.\n",
      "Start epoch 1.\n",
      "    Loss at step 400 is 0.5202123522758484.\n",
      "    Loss at step 800 is 0.3814256191253662.\n",
      "    Loss at step 1200 is 0.310028076171875.\n",
      "    Loss at step 1600 is 0.2519527077674866.\n",
      "    Loss at step 2000 is 0.25903552770614624.\n",
      "    Loss at step 2400 is 0.19799944758415222.\n",
      "    Loss at step 2800 is 0.2446938008069992.\n",
      "    Loss at step 3200 is 0.1674477756023407.\n",
      "    Loss at step 3600 is 0.20979449152946472.\n",
      "    Finished epoch 1 with average loss: 0.3153599798679352.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.1773703545331955 +/- 0.08626064658164978\n",
      "Start epoch 2.\n",
      "    Loss at step 400 is 0.15975871682167053.\n",
      "    Loss at step 800 is 0.18665388226509094.\n",
      "    Loss at step 1200 is 0.14192450046539307.\n",
      "    Loss at step 1600 is 0.17154403030872345.\n",
      "    Loss at step 2000 is 0.1467149555683136.\n",
      "    Loss at step 2400 is 0.13949400186538696.\n",
      "    Loss at step 2800 is 0.1282154619693756.\n",
      "    Loss at step 3200 is 0.13049793243408203.\n",
      "    Loss at step 3600 is 0.1320217251777649.\n",
      "    Finished epoch 2 with average loss: 0.1546495109796524.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.13422849774360657 +/- 0.06665512919425964\n",
      "Start epoch 3.\n",
      "    Loss at step 400 is 0.1204807460308075.\n",
      "    Loss at step 800 is 0.11367204785346985.\n",
      "    Loss at step 1200 is 0.10336032509803772.\n",
      "    Loss at step 1600 is 0.12745168805122375.\n",
      "    Loss at step 2000 is 0.1484886258840561.\n",
      "    Loss at step 2400 is 0.10192359238862991.\n",
      "    Loss at step 2800 is 0.11510801315307617.\n",
      "    Loss at step 3200 is 0.1152682825922966.\n",
      "    Loss at step 3600 is 0.09996055066585541.\n",
      "    Finished epoch 3 with average loss: 0.12010825425386429.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.10340648144483566 +/- 0.05573388934135437\n",
      "Start epoch 4.\n",
      "    Loss at step 400 is 0.11063528060913086.\n",
      "    Loss at step 800 is 0.09970016777515411.\n",
      "    Loss at step 1200 is 0.11319111287593842.\n",
      "    Loss at step 1600 is 0.09994981437921524.\n",
      "    Loss at step 2000 is 0.09089457988739014.\n",
      "    Loss at step 2400 is 0.11510838568210602.\n",
      "    Loss at step 2800 is 0.1062406674027443.\n",
      "    Loss at step 3200 is 0.10633593797683716.\n",
      "    Loss at step 3600 is 0.11094313114881516.\n",
      "    Finished epoch 4 with average loss: 0.10210695117712021.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.09416469931602478 +/- 0.051177795976400375\n",
      "Start epoch 5.\n",
      "    Loss at step 400 is 0.12503555417060852.\n",
      "    Loss at step 800 is 0.09440679848194122.\n",
      "    Loss at step 1200 is 0.09855256229639053.\n",
      "    Loss at step 1600 is 0.08214698731899261.\n",
      "    Loss at step 2000 is 0.10520961135625839.\n",
      "    Loss at step 2400 is 0.10043865442276001.\n",
      "    Loss at step 2800 is 0.08053530752658844.\n",
      "    Loss at step 3200 is 0.0777939036488533.\n",
      "    Loss at step 3600 is 0.08693025261163712.\n",
      "    Finished epoch 5 with average loss: 0.0905693992972374.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.08799099922180176 +/- 0.047216545790433884\n",
      "Start epoch 6.\n",
      "    Loss at step 400 is 0.08395819365978241.\n",
      "    Loss at step 800 is 0.08539425581693649.\n",
      "    Loss at step 1200 is 0.07617247104644775.\n",
      "    Loss at step 1600 is 0.0927376002073288.\n",
      "    Loss at step 2000 is 0.07501169294118881.\n",
      "    Loss at step 2400 is 0.09477798640727997.\n",
      "    Loss at step 2800 is 0.09435519576072693.\n",
      "    Loss at step 3200 is 0.0744602382183075.\n",
      "    Loss at step 3600 is 0.07618987560272217.\n",
      "    Finished epoch 6 with average loss: 0.08217592537403107.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.08026276528835297 +/- 0.04370586574077606\n",
      "Start epoch 7.\n",
      "    Loss at step 400 is 0.0778907984495163.\n",
      "    Loss at step 800 is 0.0869092047214508.\n",
      "    Loss at step 1200 is 0.07666771113872528.\n",
      "    Loss at step 1600 is 0.06897628307342529.\n",
      "    Loss at step 2000 is 0.09216386079788208.\n",
      "    Loss at step 2400 is 0.06982030719518661.\n",
      "    Loss at step 2800 is 0.08377160876989365.\n",
      "    Loss at step 3200 is 0.08825424313545227.\n",
      "    Loss at step 3600 is 0.06940186023712158.\n",
      "    Finished epoch 7 with average loss: 0.07569194585084915.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.0727429986000061 +/- 0.042023226618766785\n",
      "Start epoch 8.\n",
      "    Loss at step 400 is 0.08288067579269409.\n",
      "    Loss at step 800 is 0.057968948036432266.\n",
      "    Loss at step 1200 is 0.06478653103113174.\n",
      "    Loss at step 1600 is 0.06670410931110382.\n",
      "    Loss at step 2000 is 0.06084607541561127.\n",
      "    Loss at step 2400 is 0.061431631445884705.\n",
      "    Loss at step 2800 is 0.07967989146709442.\n",
      "    Loss at step 3200 is 0.06119172275066376.\n",
      "    Loss at step 3600 is 0.07521912455558777.\n",
      "    Finished epoch 8 with average loss: 0.0702342838048935.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.06760107725858688 +/- 0.041113462299108505\n",
      "Start epoch 9.\n",
      "    Loss at step 400 is 0.08422476798295975.\n",
      "    Loss at step 800 is 0.06464750319719315.\n",
      "    Loss at step 1200 is 0.08469433337450027.\n",
      "    Loss at step 1600 is 0.07715965807437897.\n",
      "    Loss at step 2000 is 0.06003160402178764.\n",
      "    Loss at step 2400 is 0.06768776476383209.\n",
      "    Loss at step 2800 is 0.07005563378334045.\n",
      "    Loss at step 3200 is 0.05782303214073181.\n",
      "    Loss at step 3600 is 0.06647447496652603.\n",
      "    Finished epoch 9 with average loss: 0.06564067304134369.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.06398674100637436 +/- 0.03776102513074875\n",
      "Start epoch 10.\n",
      "    Loss at step 400 is 0.06764233112335205.\n",
      "    Loss at step 800 is 0.0575503408908844.\n",
      "    Loss at step 1200 is 0.07052288949489594.\n",
      "    Loss at step 1600 is 0.051681697368621826.\n",
      "    Loss at step 2000 is 0.05034882202744484.\n",
      "    Loss at step 2400 is 0.05157409608364105.\n",
      "    Loss at step 2800 is 0.07200025767087936.\n",
      "    Loss at step 3200 is 0.059393271803855896.\n",
      "    Loss at step 3600 is 0.05373910441994667.\n",
      "    Finished epoch 10 with average loss: 0.061834610998630524.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.06140787526965141 +/- 0.03845113515853882\n",
      "Start epoch 11.\n",
      "    Loss at step 400 is 0.06107152998447418.\n",
      "    Loss at step 800 is 0.06037350371479988.\n",
      "    Loss at step 1200 is 0.06783609837293625.\n",
      "    Loss at step 1600 is 0.046866968274116516.\n",
      "    Loss at step 2000 is 0.05601330101490021.\n",
      "    Loss at step 2400 is 0.05271553993225098.\n",
      "    Loss at step 2800 is 0.07901027798652649.\n",
      "    Loss at step 3200 is 0.06178780645132065.\n",
      "    Loss at step 3600 is 0.06356181204319.\n",
      "    Finished epoch 11 with average loss: 0.058490559458732605.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.05856909230351448 +/- 0.03568544611334801\n",
      "Start epoch 12.\n",
      "    Loss at step 400 is 0.06104124337434769.\n",
      "    Loss at step 800 is 0.048059746623039246.\n",
      "    Loss at step 1200 is 0.05784022808074951.\n",
      "    Loss at step 1600 is 0.05107314512133598.\n",
      "    Loss at step 2000 is 0.05163360759615898.\n",
      "    Loss at step 2400 is 0.05897738039493561.\n",
      "    Loss at step 2800 is 0.05430550128221512.\n",
      "    Loss at step 3200 is 0.054399147629737854.\n",
      "    Loss at step 3600 is 0.055914316326379776.\n",
      "    Finished epoch 12 with average loss: 0.05548597127199173.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.056616272777318954 +/- 0.03556061536073685\n",
      "Start epoch 13.\n",
      "    Loss at step 400 is 0.04891325533390045.\n",
      "    Loss at step 800 is 0.057379983365535736.\n",
      "    Loss at step 1200 is 0.06264585256576538.\n",
      "    Loss at step 1600 is 0.045827195048332214.\n",
      "    Loss at step 2000 is 0.09137853980064392.\n",
      "    Loss at step 2400 is 0.04942859709262848.\n",
      "    Loss at step 2800 is 0.03783711791038513.\n",
      "    Loss at step 3200 is 0.04768453910946846.\n",
      "    Loss at step 3600 is 0.06771743297576904.\n",
      "    Finished epoch 13 with average loss: 0.05278055742383003.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.052256304770708084 +/- 0.03372836858034134\n",
      "Start epoch 14.\n",
      "    Loss at step 400 is 0.044116079807281494.\n",
      "    Loss at step 800 is 0.04647821560502052.\n",
      "    Loss at step 1200 is 0.047497354447841644.\n",
      "    Loss at step 1600 is 0.05498168617486954.\n",
      "    Loss at step 2000 is 0.05026121810078621.\n",
      "    Loss at step 2400 is 0.052353184670209885.\n",
      "    Loss at step 2800 is 0.047921840101480484.\n",
      "    Loss at step 3200 is 0.06472280621528625.\n",
      "    Loss at step 3600 is 0.05586976185441017.\n",
      "    Finished epoch 14 with average loss: 0.050592076033353806.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.05156037583947182 +/- 0.03335621580481529\n",
      "Start epoch 15.\n",
      "    Loss at step 400 is 0.03475155681371689.\n",
      "    Loss at step 800 is 0.047307275235652924.\n",
      "    Loss at step 1200 is 0.04861188679933548.\n",
      "    Loss at step 1600 is 0.04278101027011871.\n",
      "    Loss at step 2000 is 0.046949245035648346.\n",
      "    Loss at step 2400 is 0.04481034725904465.\n",
      "    Loss at step 2800 is 0.05405712127685547.\n",
      "    Loss at step 3200 is 0.056451909244060516.\n",
      "    Loss at step 3600 is 0.04921784996986389.\n",
      "    Finished epoch 15 with average loss: 0.04848847538232803.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.04776746779680252 +/- 0.03246725723147392\n",
      "Start epoch 16.\n",
      "    Loss at step 400 is 0.04750336706638336.\n",
      "    Loss at step 800 is 0.06505003571510315.\n",
      "    Loss at step 1200 is 0.040982961654663086.\n",
      "    Loss at step 1600 is 0.052853651344776154.\n",
      "    Loss at step 2000 is 0.04142548143863678.\n",
      "    Loss at step 2400 is 0.046857673674821854.\n",
      "    Loss at step 2800 is 0.04206079989671707.\n",
      "    Loss at step 3200 is 0.045265402644872665.\n",
      "    Loss at step 3600 is 0.03996092826128006.\n",
      "    Finished epoch 16 with average loss: 0.04668084532022476.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.04850628226995468 +/- 0.0312040988355875\n",
      "Start epoch 17.\n",
      "    Loss at step 400 is 0.0339711531996727.\n",
      "    Loss at step 800 is 0.047333430498838425.\n",
      "    Loss at step 1200 is 0.04497275501489639.\n",
      "    Loss at step 1600 is 0.040872927755117416.\n",
      "    Loss at step 2000 is 0.05353555083274841.\n",
      "    Loss at step 2400 is 0.04280461370944977.\n",
      "    Loss at step 2800 is 0.049397505819797516.\n",
      "    Loss at step 3200 is 0.044329866766929626.\n",
      "    Loss at step 3600 is 0.04537094384431839.\n",
      "    Finished epoch 17 with average loss: 0.04504413902759552.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.04686109349131584 +/- 0.03159619867801666\n",
      "Start epoch 18.\n",
      "    Loss at step 400 is 0.0398838073015213.\n",
      "    Loss at step 800 is 0.04956860840320587.\n",
      "    Loss at step 1200 is 0.04284754395484924.\n",
      "    Loss at step 1600 is 0.0437883585691452.\n",
      "    Loss at step 2000 is 0.055092357099056244.\n",
      "    Loss at step 2400 is 0.036466699093580246.\n",
      "    Loss at step 2800 is 0.041832808405160904.\n",
      "    Loss at step 3200 is 0.039747461676597595.\n",
      "    Loss at step 3600 is 0.033361706882715225.\n",
      "    Finished epoch 18 with average loss: 0.0435732938349247.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.04347532242536545 +/- 0.030028583481907845\n",
      "Start epoch 19.\n",
      "    Loss at step 400 is 0.05087413638830185.\n",
      "    Loss at step 800 is 0.032092221081256866.\n",
      "    Loss at step 1200 is 0.03840364143252373.\n",
      "    Loss at step 1600 is 0.05194439738988876.\n",
      "    Loss at step 2000 is 0.043371886014938354.\n",
      "    Loss at step 2400 is 0.043327637016773224.\n",
      "    Loss at step 2800 is 0.043482862412929535.\n",
      "    Loss at step 3200 is 0.04214339703321457.\n",
      "    Loss at step 3600 is 0.03013090044260025.\n",
      "    Finished epoch 19 with average loss: 0.042229220271110535.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.04305681213736534 +/- 0.030310701578855515\n",
      "Start epoch 20.\n",
      "    Loss at step 400 is 0.044074490666389465.\n",
      "    Loss at step 800 is 0.037035923451185226.\n",
      "    Loss at step 1200 is 0.03599470853805542.\n",
      "    Loss at step 1600 is 0.04948985576629639.\n",
      "    Loss at step 2000 is 0.04508401080965996.\n",
      "    Loss at step 2400 is 0.03540971502661705.\n",
      "    Loss at step 2800 is 0.04022101312875748.\n",
      "    Loss at step 3200 is 0.03717825561761856.\n",
      "    Loss at step 3600 is 0.034278325736522675.\n",
      "    Finished epoch 20 with average loss: 0.04102078825235367.\n",
      "    Start Validation Loop\n",
      "    validation loss: 0.04101375490427017 +/- 0.029095500707626343\n",
      "Finished training.\n",
      "Uploading model to wandb.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a50a5895e3468dbf449a61381f21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='69.657 MB of 69.657 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/MSE_on_fixed_grid</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/MSE_on_fixed_grid_std</td><td>█▇▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>batch/loss</td><td>█▆▄▄▄▄▃▃▃▄▄▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss_std_over_100_steps</td><td>█▇▆▅▆▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>batch/step_within_epoch</td><td>▂▅▆▆▇▂▃▄▅▇▇▂█▆▂▅█▄▄▇▄▁▇▂▂▆▇▂▃▃▆▇▆▇▂▂▆▁▆█</td></tr><tr><td>batch_within_epoch</td><td>▂▂▃▃▃▁▂▃▆▅▅▇▃▂▇█▁▁▂▃▆▆▄▆▆▂▂▂▅▅█▁▂▇▄▇▇▅▅█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇███████</td></tr><tr><td>epoch/loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/validation/loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>epoch/validation/loss-std</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/MSE_on_fixed_grid</td><td>0.00496</td></tr><tr><td>batch/MSE_on_fixed_grid_std</td><td>0.00288</td></tr><tr><td>batch/global_step</td><td>75000</td></tr><tr><td>batch/loss</td><td>0.03992</td></tr><tr><td>batch/loss_std_over_100_steps</td><td>0.00625</td></tr><tr><td>batch/step_within_epoch</td><td>3749</td></tr><tr><td>batch_within_epoch</td><td>3750</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>epoch/loss</td><td>0.04102</td></tr><tr><td>epoch/validation/loss</td><td>0.04101</td></tr><tr><td>epoch/validation/loss-std</td><td>0.0291</td></tr><tr><td>used_config</td><td>{'after_epoch_callba...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-elevator-38</strong> at: <a href='https://wandb.ai/nld/inr_edu_24/runs/isds2pb9' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/isds2pb9</a><br/> View project at: <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">https://wandb.ai/nld/inr_edu_24</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 160 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240914_141202-isds2pb9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(\n",
    "    project='inr_edu_24',\n",
    "    notes='hypernetwork test',\n",
    "    tags=('test',)\n",
    ") as run:\n",
    "    run.log({'used_config': pprint.pformat(config)})\n",
    "    results = trainer.train(next(key_gen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
